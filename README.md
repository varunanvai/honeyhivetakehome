This is my repo for the HoneyHive.AI take-home assignment.

System Architecture and Workflow Description:
Step 1. User passes input that contains the python pipeline code block, the list of inputs, and the metrics code blocks into API Gateway by hitting a given endpoint that we have provided them.  
Example JSON body to be sent to API Gateway
{
  "job_id": "11111111",
  "email" : "fafafadf@gmail.com",
  "pipeline_code_block": "def pipeline(config,inputs) ...." ,
  "inputs" = [ 
	{
		"topic": "enterprise SaaS pitch",
		"input_metadata": {
			"importance": 0.9
		}
	},
	{
		"topic": "marketing campaign",
		"input_metadata": {
			"importance": 0.5
		}
	},
 â€¦ ]
  "metrics": [
    "metrics_code_block_1": "def email_quality(output):
   return openai.ChatCompletion.create(...)",
   ...
  ]
}
Step 2: API gateway will take in the user input and pass it on to a Lambda function that will handle formatting and validation of the inputs into a JSON that can be used by the Step Function that will then handle the calling and parallelization of four main lambda functions that will handle the four subtasks required for this service. Each input will be given a unique identifier or ID for later storage management and tracking. The API Gateway will be configured to take in POST requests and have the lambda arn of the formatting and validation lambda as well as specifying the two different responses: a 200 for a successfully formatted and validated input and a 400 if the input is deemed invalid or impossible to properly format. 
Example lambda code_block

def lambda_handler(event, context):
  input = json.loads(event['body'])

  # Format the input
  # ...

  # If the input is valid determined by some validation function, start a Step Functions execution
  if is_valid_input(input):
    step_functions_client = boto3.client('stepfunctions')

    step_functions_client.start_execution(
      stateMachineArn='YOUR_STEP_FUNCTION_ARN',
      input=json.dumps(input)
    )

    return {
      'statusCode': 200,
      'body': json.dumps('Step Function execution started successfully.')
    }
  else:
    return {
      'statusCode': 400,
      'body': json.dumps('Invalid input.')
    }
Step 3: AWS Step Functions is in charge of the handling of four different lambdas to handle the four different processes needed for this service and will take in the formatted inputs from the previous Lambda.
It will then pass in the pipeline codeblock and the specified inputs to the first lambda that will then send it as JSON to the SQS for job queueing. All SQS's in our system will have a corresponding DLQ configured so failed messages will go to a DLQ, so we can redrive them after fixing whatever the issue maybe.
Lambda code block
def lambda_handler(event, context):
  input = json.loads(event['body'])

  sqs_client = boto3.client('sqs')

  sqs_client.send_message(
    QueueUrl='YOUR_SQS_QUEUE_URL',
    MessageBody=json.dumps(input)
  )

  return {
    'statusCode': 200,
    'body': json.dumps('Job sent to SQS successfully.')
  }
Step 4: The second Lambda is subscribed to the SQS and polls for it everytime a message comes into the queue. It will then take that message and start running the pipeline codeblock for each input on a dedicated EC2 instance that will have all necessary software and packages installed. The code in the EC2 instance will then write the outputs of the pipeline to a dedicated S3 storage bucket in a folder labeled by its jobID with a subfolder for each input labeled by its unique identifier. On upload, there will be a trigger for an S3 event that will send a message to a second SQS similar to the first one that contains the bucket name and file path to the output data. For all EC2 instances used in this system, we can consider using a ECS or EKS to help better scale and manage the different instances. 
Example code for lambda 

def lambda_handler(event, context):
sqs_client = boto3.client('sqs')

ec2_client = boto3.client('ec2')

messages = sqs_client.receive_message()

if messages['Messages']:
    for message in messages['Messages']:
        # Get the job input from the message
        job_input = json.loads(message['Body'])

        # The script hosted on ec2 will write the outputs to S3 storage under a folder labeled with the jobId. 
        ec2_client.run_instances(pass in pipeline codeblock and inputs and output path to write the outputs to s3 for each input)

        # Delete the message from the SQS queue
        sqs_client.delete_message(...)

return {
    'statusCode': 200,
    'body': json.dumps('Messages processed successfully.')
} 
Step 5: A third Lambda function will be listening to the second SQS and when it has an update, it will read the output from s3 and run the metrics codeblocks on it for each input in its own dedicated EC2 instances. These metrics codeblocks will be aggregated into a report and stored in S3 for future analytics and safekeeping. 
Example code for lambda 
def lambda_handler(event, context):
sqs_client = boto3.client('sqs')

ec2_client = boto3.client('ec2')

messages = sqs_client.receive_message()

if messages['Messages']:
    for message in messages['Messages']:
        # Get the output from the previous Lambda function
        output = json.loads(message['Body'])

        # Start a script on the EC2 instance to run the metrics codeblocks
        ec2_client.run_instances(this will take in the metrics codeblocks as well as the output path to write the report to in s3)

        # Delete the message from the SQS queue
        sqs_client.delete_message()

return {
    'statusCode': 200,
    'body': json.dumps('Messages processed successfully.')
}
Step 6: We will have third SQS listening for an S3 event on upload of a report in the same way the previous SQS's did. The report will be a JSON containing all the necessary metrics data that will then be put into the final report email. The last lambda function will be listening to the SQS and upon each new update, the lambda will format the email and give it to the SES for final email delivery of the report. The report will be formatted dynamically, generating the right number of fields based on how many metrics the user provided and will use the metadata to prioritize which input results to show first based on input importance metadata, whether or not the individual process failed for that input, as well as possibly including other analytics that we run on the data. 
Example code block for lambda
def lambda_handler(event, context):
  sqs_client = boto3.client('sqs')

  sns_client = boto3.client('sns')

  messages = sqs_client.receive_message(...)

  for message in messages['Messages']:

    file_path = message['Body']

    #code to get metrics data from s3 file path
    # ...

    # Format the reports data into an dynamic email body with pre-formatted sections at the top and bottom
    # and dynamically organized for each input in the total job
    email_body = ...

    ses_client.send_email(...)

    sqs_client.delete_message(...)

  # Return a success response
  return {
    'statusCode': 200,
    'body': json.dumps('Messages processed successfully.')
  }


This was the final step of my architecture. Now for just some general notes.
Asynchronicity: This is largely asynchronous system architecture due to use of lambdas and SQS's that allow the processes within the system to be decoupled and run independently from each other. Lambda allows for concurrent executions and can handle different users requests at the same time and the SQS help create a list of tasks for the lambdas to perform asynchronously. EC2 also aids this because the Lambda functions can start the EC2 instances and then continue running without having to wait for the EC2 instances to finish their tasks.
Error handling: I believe this is a very robust system that can properly deal with failures and errors gracefully and swiftly. Validating at the very beginning ensures that we immediately let the user know if there is a problem with their input or not rather than waiting for one of the subprocesses to fail because of bad input. The SQS's ensure that if a Lambda function fails, the messages in the SQS queue will not be lost. We will also configure our SQS as previously stated to use dead letter queues, whose benefit was described earlier as well as configure them to perform retries to make the system more resilient. We will also use AWS CloudWatch to get good logs and monitor the health of our systems. The use of EC2 instances is also good for these goals because they are more robust to failures than just purely using Lambdas. 
